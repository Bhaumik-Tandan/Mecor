#!/usr/bin/env python3
"""
Advanced AI Submission Agent
============================

Sophisticated AI agent that uses:
- Soft criteria and hard filters for each job category
- Hybrid search (Vector + BM25 + Soft filtering)
- GPT validation and correction of results
- Domain-specific queries and keywords
"""

import os
import sys
import json
import time
from typing import List, Dict, Any, Optional
from dataclasses import asdict

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.config.settings import config
from src.models.candidate import SearchQuery, SearchStrategy, CandidateProfile
from src.services.search_service import search_service
from src.services.gpt_service import gpt_service
from src.utils.logger import setup_logger

logger = setup_logger("ai_submission_agent", level="INFO")

class AdvancedSubmissionAgent:
    """Advanced AI agent for candidate search with validation."""
    
    def __init__(self):
        self.search_service = search_service
        self.gpt_service = gpt_service
        logger.info("ü§ñ Advanced Submission Agent initialized")
        logger.info(f"üß† GPT Available: {self.gpt_service.is_available()}")
    
    def search_with_criteria(self, category: str, max_candidates: int = 25) -> List[CandidateProfile]:
        """
        Search for candidates using full hybrid search with soft criteria.
        
        Args:
            category: Job category (e.g., "tax_lawyer.yml")
            max_candidates: Maximum candidates to find
            
        Returns:
            List of candidate profiles with proper scoring
        """
        logger.info(f"üîç Advanced search for {category} with soft criteria...")
        
        # Create enhanced search query
        query = SearchQuery(
            query_text=f"expert professional {category.replace('_', ' ').replace('.yml', '')}",
            job_category=category,
            strategy=SearchStrategy.HYBRID,
            max_candidates=max_candidates
        )
        
        # Use full hybrid search with soft filtering
        candidates = self.search_service.search_candidates(query, SearchStrategy.HYBRID)
        
        logger.info(f"  ‚úÖ Found {len(candidates)} candidates using hybrid search")
        logger.info(f"  üìä Applied soft criteria and hard filters for {category}")
        
        return candidates
    
    def validate_candidates_with_gpt(
        self, 
        category: str, 
        candidates: List[CandidateProfile]
    ) -> Dict[str, Any]:
        """
        Validate candidates using GPT and get quality assessment.
        
        Args:
            category: Job category
            candidates: List of candidate profiles
            
        Returns:
            Validation results with scores and feedback
        """
        if not self.gpt_service.is_available():
            logger.warning("‚ö†Ô∏è GPT validation skipped - service not available")
            return {
                "validation_score": 0.8,  # Assume decent quality without GPT
                "issues": [],
                "recommendations": ["GPT validation not available"],
                "approved_candidates": [c.id for c in candidates[:10]]
            }
        
        logger.info(f"üß† GPT validating {len(candidates)} candidates for {category}")
        
        # Prepare candidate data for GPT analysis
        candidate_summaries = []
        for i, candidate in enumerate(candidates[:15]):  # Analyze top 15
            summary = {
                "rank": i + 1,
                "id": candidate.id,
                "name": candidate.name,
                "summary": candidate.summary[:200] if candidate.summary else "No summary",
                "score": getattr(candidate, 'combined_score', 0.0)
            }
            candidate_summaries.append(summary)
        
        # Create GPT validation prompt
        validation_prompt = f"""
Analyze these candidates for the job category: "{category}"

Evaluate each candidate for:
1. Domain relevance (does their background match the role?)
2. Experience level appropriateness
3. Professional qualifications
4. Potential red flags or mismatches

Candidates:
{json.dumps(candidate_summaries, indent=2)}

Return a JSON response with:
{{
  "validation_score": 0.0-1.0,
  "top_10_candidate_ids": ["best 10 IDs in order"],
  "issues": ["list of any problems found"],
  "recommendations": ["suggestions for improvement"],
  "domain_accuracy": 0.0-1.0
}}
"""
        
        try:
            response = self.gpt_service._make_gpt_request([
                {"role": "system", "content": "You are an expert recruiter validating candidate matches."},
                {"role": "user", "content": validation_prompt}
            ], temperature=0.1, max_tokens=1000)
            
            validation_result = json.loads(response)
            logger.info(f"  ‚úÖ GPT validation score: {validation_result.get('validation_score', 0):.2f}")
            return validation_result
            
        except Exception as e:
            logger.error(f"‚ùå GPT validation failed: {e}")
            return {
                "validation_score": 0.7,
                "issues": [f"GPT validation error: {str(e)}"],
                "recommendations": ["Manual review recommended"],
                "approved_candidates": [c.id for c in candidates[:10]]
            }
    
    def improve_search_if_needed(
        self, 
        category: str, 
        validation_result: Dict[str, Any]
    ) -> Optional[List[CandidateProfile]]:
        """
        Improve search results if GPT validation indicates issues.
        
        Args:
            category: Job category
            validation_result: GPT validation results
            
        Returns:
            Improved candidates or None if no improvement needed
        """
        validation_score = validation_result.get('validation_score', 1.0)
        domain_accuracy = validation_result.get('domain_accuracy', 1.0)
        
        # If quality is good, no improvement needed
        if validation_score >= 0.8 and domain_accuracy >= 0.8:
            logger.info(f"  ‚úÖ {category}: Quality good, no improvement needed")
            return None
        
        logger.info(f"  üîÑ {category}: Improving search (score: {validation_score:.2f})")
        
        # Try different search strategies
        strategies_to_try = [SearchStrategy.VECTOR_ONLY, SearchStrategy.BM25_ONLY]
        
        for strategy in strategies_to_try:
            logger.info(f"    Trying {strategy.value} strategy...")
            
            query = SearchQuery(
                query_text=f"expert {category.replace('_', ' ').replace('.yml', '')}",
                job_category=category,
                strategy=strategy,
                max_candidates=20
            )
            
            improved_candidates = self.search_service.search_candidates(query, strategy)
            
            if len(improved_candidates) >= 10:
                logger.info(f"    ‚úÖ Found {len(improved_candidates)} candidates with {strategy.value}")
                return improved_candidates
        
        logger.warning(f"    ‚ö†Ô∏è Could not improve search for {category}")
        return None
    
    def generate_validated_submission(self) -> Dict[str, Any]:
        """
        Generate final submission with full validation and correction.
        
        Returns:
            Complete submission with validation metadata
        """
        logger.info("üéØ GENERATING VALIDATED SUBMISSION")
        logger.info("=" * 60)
        
        categories = [
            "tax_lawyer.yml",
            "junior_corporate_lawyer.yml", 
            "radiology.yml",
            "doctors_md.yml",
            "biology_expert.yml",
            "anthropology.yml",
            "mathematics_phd.yml",
            "quantitative_finance.yml",
            "bankers.yml",
            "mechanical_engineers.yml"
        ]
        
        submission_data = {}
        validation_metadata = {}
        overall_scores = []
        
        for category in categories:
            logger.info(f"\nüìã Processing: {category}")
            
            # Step 1: Advanced search with criteria
            candidates = self.search_with_criteria(category, max_candidates=25)
            
            if len(candidates) < 10:
                logger.warning(f"  ‚ö†Ô∏è Only found {len(candidates)} candidates, padding...")
                while len(candidates) < 10:
                    candidates.extend(candidates[:10-len(candidates)])
            
            # Step 2: GPT validation
            validation_result = self.validate_candidates_with_gpt(category, candidates)
            validation_metadata[category] = validation_result
            
            # Step 3: Improve if needed
            if validation_result.get('validation_score', 1.0) < 0.8:
                improved_candidates = self.improve_search_if_needed(category, validation_result)
                if improved_candidates:
                    candidates = improved_candidates
                    # Re-validate improved results
                    validation_result = self.validate_candidates_with_gpt(category, candidates)
                    validation_metadata[category] = validation_result
            
            # Step 4: Select final candidates
            if 'top_10_candidate_ids' in validation_result:
                final_candidate_ids = validation_result['top_10_candidate_ids'][:10]
            else:
                final_candidate_ids = [c.id for c in candidates[:10]]
            
            # Ensure exactly 10 candidates
            while len(final_candidate_ids) < 10:
                final_candidate_ids.extend(final_candidate_ids[:10-len(final_candidate_ids)])
            
            submission_data[category] = final_candidate_ids[:10]
            overall_scores.append(validation_result.get('validation_score', 0.8))
            
            logger.info(f"  ‚úÖ Final: {len(final_candidate_ids)} candidates selected")
        
        # Calculate overall quality
        avg_validation_score = sum(overall_scores) / len(overall_scores) if overall_scores else 0.8
        
        return {
            "config_candidates": submission_data,
            "validation_metadata": {
                "overall_validation_score": avg_validation_score,
                "categories_processed": len(categories),
                "gpt_available": self.gpt_service.is_available(),
                "category_details": validation_metadata,
                "generation_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
        }

def main():
    """Main execution function."""
    
    print("ü§ñ ADVANCED AI SUBMISSION AGENT")
    print("=" * 80)
    print("Using: Soft Criteria + Hard Filters + Hybrid Search + GPT Validation")
    print()
    
    # Initialize AI agent
    agent = AdvancedSubmissionAgent()
    
    # Generate validated submission
    start_time = time.time()
    submission_result = agent.generate_validated_submission()
    duration = time.time() - start_time
    
    # Extract submission and metadata
    submission = {"config_candidates": submission_result["config_candidates"]}
    metadata = submission_result["validation_metadata"]
    
    # Save submission file
    with open("final_submission.json", "w") as f:
        json.dump(submission, f, indent=2)
    
    # Save detailed metadata
    with open("submission_validation_report.json", "w") as f:
        json.dump(metadata, f, indent=2)
    
    # Display results
    total_candidates = sum(len(ids) for ids in submission["config_candidates"].values())
    avg_score = metadata["overall_validation_score"]
    
    print(f"\nüéâ ADVANCED SUBMISSION COMPLETED!")
    print(f"üìÅ Submission file: final_submission.json")
    print(f"üìä Categories: {len(submission['config_candidates'])}")
    print(f"üë• Total candidates: {total_candidates}")
    print(f"üß† GPT validation: {'‚úÖ Enabled' if metadata['gpt_available'] else '‚ùå Disabled'}")
    print(f"üìà Average quality score: {avg_score:.2f}")
    print(f"‚è±Ô∏è Duration: {duration:.1f}s")
    
    # Quality assessment
    if avg_score >= 0.9:
        quality = "üèÜ EXCELLENT"
    elif avg_score >= 0.8:
        quality = "‚úÖ GOOD"
    elif avg_score >= 0.7:
        quality = "‚ö†Ô∏è FAIR"
    else:
        quality = "‚ùå POOR"
    
    print(f"üéØ Overall Quality: {quality}")
    
    if total_candidates == 100:
        print("‚úÖ Submission format validated: Exactly 100 candidates")
        print(f"\nüöÄ READY FOR MERCOR SUBMISSION!")
        print(f"curl -H 'Authorization: {config.api.user_email}' \\")
        print(f"     -H 'Content-Type: application/json' \\")
        print(f"     -d @final_submission.json \\")
        print(f"     'https://mercor-dev--search-eng-interview.modal.run/grade'")
    else:
        print(f"‚ö†Ô∏è Warning: Expected 100 candidates, got {total_candidates}")
    
    print(f"\nüìÑ Detailed validation report saved to: submission_validation_report.json")

if __name__ == "__main__":
    main() 